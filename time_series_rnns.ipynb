{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Time Series</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d48fb06523b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# let's see the progress bar\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import tensorflow as tf\n",
    "# let's see the progress bar\n",
    "from tqdm import tqdm\n",
    "tqdm().pandas()\n",
    "import seaborn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../Datasets/kep_lightcurves.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>How many stars do we have here ?</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stars = df.columns\n",
    "stars = list(set([i.split(\"_\")[0] for i in stars]))\n",
    "print(f\"The number of stars available is: {len(stars)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stars:\n",
    "    name = i+\"_rscl\"\n",
    "    plt.figure(figsize=(35,10))\n",
    "    plt.plot(df.index[:50000],df[name][:50000],'b+')\n",
    "    plt.title(\"Star \"+i)\n",
    "    plt.xlabel(\"Rescaled flux\")\n",
    "    plt.ylabel(\"Normalized time\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"lightcurve_\"+i+\".jpeg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty amazing no? Like we see, the behavior of the flux for each star is very different, this is very exciting to test different models to predict the holes in the time series. But, most of this flux cannot be used directly. We need to clean a little the data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Clean the data</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h3>Mean on predefined number of point</h3></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_windows(time, flux, lag=5):\n",
    "    '''\n",
    "    This function denoise the data naively by make a mean between lag number points.\n",
    "    @param time: (list) list of time values\n",
    "    @param flux: (list) list of floats -> flux of the star\n",
    "    @param lag: (int) number of points for the mean, default 5\n",
    "    @return X: (list) time rescaled\n",
    "    @return y: (list) flux rescaled by mean \n",
    "    @return y_std: (list) list of standard deviations for each rescaled points\n",
    "    '''\n",
    "    # let's do some simple code\n",
    "    # Empty lists\n",
    "    X = []\n",
    "    y = []\n",
    "    y_std = []\n",
    "    j = 0 # increment\n",
    "    for i in range(int(len(flux)/lag)):\n",
    "        X.append(np.mean(time[(i+j):(i+j+lag)]))\n",
    "        y.append(np.mean(flux[(i+j):(i+j+lag)]))\n",
    "        y_std.append(np.std(flux[(i+j):(i+j+lag)]))\n",
    "        j+= lag\n",
    "    \n",
    "    \n",
    "    return X, y, y_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats a pretty simple function, you pass your data and specified the number of points you want to use for the mean. Now let's take a look with one noisy star (not too noisy) the star 001724719."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the number of points with the mean on 10 points\n",
    "x, y, y_err  = mean_windows(df.index,df[\"001724719_rscl\"], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plot the results, the orignal data will be in blue, the reduced data will be in red. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = [1,2,11]\n",
    "for i in list_:\n",
    "    name = stars[i]+\"_rscl\"\n",
    "    x, y, y_err  = mean_sliding_windows(df.index[25000:30000],df[name][25000:30000], 40)\n",
    "    plt.figure(figsize=(35,10))\n",
    "   \n",
    "    plt.plot(df.index[25000:30000],df[name][25000:30000],'b+', label=\"Brut\")\n",
    "    plt.plot(x,y, 'r-.', label=\"Rescaled\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Star \"+stars[i]+\"_rscl\")\n",
    "    plt.xlabel(\"Rescaled flux\")\n",
    "    plt.ylabel(\"Normalized time\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(stars[i]+\"_rscl_mean_window.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35,10))\n",
    "plt.plot(df.index,df[\"001724719_rscl\"],'b+', label=\"Brut\")\n",
    "plt.plot(x,y, 'r-.', label=\"Rescaled\")\n",
    "plt.legend()\n",
    "plt.title(\"Star \"+i)\n",
    "plt.xlabel(\"Rescaled flux\")\n",
    "plt.ylabel(\"Normalized time\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, not very lisible... Let's zoom in a little, plot the first 10,000 points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35,10))\n",
    "plt.plot(df.index[:10000],df[\"001724719_rscl\"][:10000],'b+', label=\"Brut\")\n",
    "plt.plot(x[:910],y[:910], 'r-.', label=\"Rescaled\")\n",
    "plt.legend()\n",
    "plt.title(\"Star \"+i)\n",
    "plt.xlabel(\"Rescaled flux\")\n",
    "plt.ylabel(\"Normalized time\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even better, but we always have difficulties to see the result. Let's zoom in again and just show the 2,000 first points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35,10))\n",
    "plt.plot(df.index[:2000],df[\"001724719_rscl\"][:2000],'b+', label=\"Brut\")\n",
    "plt.plot(x[:182],y[:182], 'r-.', label=\"Rescaled\")\n",
    "plt.legend()\n",
    "plt.title(\"Star \"+i)\n",
    "plt.xlabel(\"Rescaled flux\")\n",
    "plt.ylabel(\"Normalized time\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look very good. We can also test other methods to practice this denoising."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><h3>Sliding window</h3></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_sliding_windows(time, flux, lag=5):\n",
    "    '''\n",
    "    This function denoise the data naively by sliding a window and make a mean between the lag number points.\n",
    "    @param time: (list) list of time values\n",
    "    @param flux: (list) list of floats -> flux of the star\n",
    "    @param lag: (int) number of points for the mean, default 5\n",
    "    @return X: (list) time rescaled\n",
    "    @return y: (list) flux rescaled by mean \n",
    "    @return y_std: (list) list of standard deviations for each rescaled points\n",
    "    '''\n",
    "    # let's do some simple code\n",
    "    # Empty lists\n",
    "    X = []\n",
    "    y = []\n",
    "    y_std = []\n",
    "    j = 0 # increment\n",
    "    for i in range(int(len(flux)-lag)):\n",
    "        \n",
    "        _flux = flux[i:(i+lag)]\n",
    "        _time = time[i:(i+lag)]\n",
    "        #_ind = [x[0] for x in enumerate(_flux) if str(x[1]) != 'nan']\n",
    "        #_time = [np.array(_time)[k] for k in _ind]\n",
    "        #_flux = [np.array(_flux)[k] for k in _ind]\n",
    "        #X.append(np.mean(_time) if len(_time)>1 else (_time[0] if len(_time) >0 else np.nan))\n",
    "        #y.append(np.mean(_flux) if len(_flux)>1 else (_flux[0] if len(_flux) >0 else np.nan))\n",
    "        #y_std.append(np.std(_flux) if len(_flux)>1 else (_flux[0] if len(_flux) >0 else np.nan))\n",
    "        X.append(np.mean(_time))\n",
    "        y.append(np.mean(_flux))\n",
    "        y_std.append(np.std(_flux))         \n",
    "        \n",
    "        j+= 1\n",
    "        \n",
    "    \n",
    "        \n",
    "    return X, y, y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the number of points with the mean on 10 points\n",
    "x_slide, y_slide, y_err_slide  = mean_sliding_windows(df.index,df[\"001724719_rscl\"], 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x), len(x_slide), len(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(35,15))\n",
    "plt.plot(df.index[:3000],df[\"001724719_rscl\"][:3000],'b+', label=\"Brut\")\n",
    "plt.plot(x[:273],y[:273], 'g-+', label=\"Rescaled mean\")\n",
    "plt.plot(x_slide[:2980],y_slide[:2980], 'r-.', label=\"Rescaled slide\")\n",
    "plt.legend()\n",
    "plt.title(\"Star \"+i)\n",
    "plt.xlabel(\"Rescaled flux\")\n",
    "plt.ylabel(\"Normalized time\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting because the slide window permit to keep small fluctuations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, here are two methods, we could make a function and apply it to the whole dataset and save into a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduced_data(df,stars):\n",
    "    '''\n",
    "    Function to automatically reduced a dataset \n",
    "    @param df: (pandas dataframe) dataframe containing all the data\n",
    "    @param stars: (list) list containing the name of each stars we want reduced data\n",
    "    @return df_mean: dataframe containing the data reduced by the mean function\n",
    "    @return df_slide: dataframe containing the data reduced by the sliding window method\n",
    "    '''\n",
    "    df_mean = pd.DataFrame()\n",
    "    df_slide = pd.DataFrame()\n",
    "    for i in tqdm(stars):\n",
    "        \n",
    "        x , y, y_std = mean_windows(df.index, df[i+\"_rscl\"], lag=25)\n",
    "        df_mean[i+\"_rscl_x\"] = x\n",
    "        df_mean[i+\"_rscl_y\"] = y\n",
    "        df_mean[i+\"_rscl_y_std\"] = y_std\n",
    "        \n",
    "        x , y, y_std = mean_sliding_windows(df.index, df[i+\"_rscl\"], lag=40)\n",
    "        df_slide[i+\"_rscl_x\"]= x\n",
    "        df_slide[i+\"_rscl_y\"]= y\n",
    "        df_slide[i+\"_rscl_y_std\"]= y_std\n",
    "    \n",
    "    return df_mean, df_slide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean, df_slide = reduced_data(df,stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stars:\n",
    "    name = i+\"_rscl\"\n",
    "    plt.figure(figsize=(35,10))\n",
    "    plt.plot(df.index[25000:35000],df[name][25000:35000],'b+', label=\"Brut\")\n",
    "    plt.plot(df_mean[name+\"_x\"][807:1125],df_mean[name+\"_y\"][807:1125],'g-+', label=\"Rescaled mean\")\n",
    "    plt.plot(df_slide[name+\"_x\"][25000:35000],df_slide[name+\"_y\"][25000:35000],'r-.', label=\"Rescaled window\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Star \"+i)\n",
    "    plt.xlabel(\"Rescaled flux\")\n",
    "    plt.ylabel(\"Normalized time\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty good don't you ? Let's zoom a little. Printing just 5,000 points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stars:\n",
    "    name = i+\"_rscl\"\n",
    "    plt.figure(figsize=(35,10))\n",
    "    plt.plot(df.index[25000:30000],df[name][25000:30000],'b+', label=\"Brut\")\n",
    "    plt.plot(df_mean[name+\"_x\"][807:968],df_mean[name+\"_y\"][807:968],'g+', label=\"Rescaled mean\")\n",
    "    plt.plot(df_slide[name+\"_x\"][25000:29975],df_slide[name+\"_y\"][25000:29975],'r.', label=\"Rescaled window\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Star \"+i)\n",
    "    plt.xlabel(\"Rescaled flux\")\n",
    "    plt.ylabel(\"Normalized time\")\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that all the data cannot be reduced with the same parameters. Certain curve can be more smooth. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look closer, the star 010024701 has just few points. We need to rescale this star. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in stars[2:3]:\n",
    "    name = i+\"_rscl\"\n",
    "    plt.figure(figsize=(35,10))\n",
    "    plt.plot(df.index[20000:28700],df[name][20000:28700],'b+', label=\"Brut\")\n",
    "    plt.plot(df_mean[name+\"_x\"][650:900],df_mean[name+\"_y\"][650:900],'g-+', label=\"Rescaled mean\")\n",
    "    plt.plot(df_slide[name+\"_x\"][20000:27975],df_slide[name+\"_y\"][20000:27975],'r-.', label=\"Rescaled window\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Star \"+i)\n",
    "    plt.xlabel(\"Rescaled flux\")\n",
    "    plt.ylabel(\"Normalized time\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"ligthcurve_\"+i+\".jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Forecasting with Machine Learning</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need two forecast in this data, if you look with attention you'll see micro holes and big holes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_mean.copy()\n",
    "df_model2= df_slide.copy()\n",
    "#df_model[stars[9]+\"_rscl\"].fillna((0), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = df_model[stars[2]+\"_rscl_y\"][:27000].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><u><i>Rescaled index</i></u></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.date_range('2009-03-07', periods=len(df.index), freq='h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>Data preparation</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# convert an array of values into a dataset matrix\n",
    "def create_dataset(values, look_back=1):\n",
    "    '''\n",
    "    Function to prepare a list of (x, y) data points to data for time series learning\n",
    "    @param values: (list) list of values \n",
    "    @param look_back: (int) number of values for the x list [x1, x2, x3, ... , xn] default 1\n",
    "    @return _x: x values for the time series\n",
    "    @return _y: y values for the time series \n",
    "    '''\n",
    "    # set empty lists \n",
    "    _x, _y = [], []\n",
    "    for i in range(len(values)-look_back-1):\n",
    "        a = values[i:(i+look_back)]      # stack a list of values\n",
    "        _x.append(a)                        # set x\n",
    "        _y.append(values[i + look_back]) # set y\n",
    "    return np.array(_x), np.array(_y)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num = 2\n",
    "values = df_model[stars[num]+\"_rscl_y\"].values\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(values[~np.isnan(values)].reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "train_size = int(len(dataset) * 0.8)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[:train_size], dataset[train_size:]\n",
    "# reshape into X=t and Y=t+1\n",
    "look_back = 20\n",
    "train_x, train_y = create_dataset(train, look_back)\n",
    "test_x, test_y = create_dataset(test, look_back)\n",
    "# reshape input to be [samples, time steps, features]\n",
    "train_x = np.reshape(train_x, (train_x.shape[0], train_x.shape[1], 1))\n",
    "test_x = np.reshape(test_x, (test_x.shape[0], test_x.shape[1], 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>Functions to generate DL models and plot</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def metrics_time_series(y_true, y_pred):\n",
    "    '''\n",
    "    Compute the MAE and MSE metrics from sklearn.metrics\n",
    "    @param y_true: (list) list of the true values\n",
    "    @param y_pred: (list) list of predicted values\n",
    "    @return mae, mse: (float), (float) values of metrics MAE and MSE \n",
    "    '''\n",
    "    mae = round(mean_absolute_error(y_true, y_pred), 2)\n",
    "    mse = round(mean_squared_error(y_true, y_pred), 2)\n",
    "    print(f\"The mean absolute error is: {mae}\")\n",
    "    print(f\"The mean squared error is: {mse}\")\n",
    "    return mae, mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_deep_learning(x_train, y_train, x_test, y_test, model_dl=LSTM ,  unit=4, look_back=20, cnn=False, bidirection=False, stacked=False):\n",
    "    '''\n",
    "    Generate RNN models with different combinations. Can be simple, stacked or bidirectional. Models can also be used with a CNN part. \n",
    "    @param x_train: (matrix) x training data with dimension (samples, time steps, features)\n",
    "    @param y_train: (list) predict data \n",
    "    @param x_test: (matrix) x testing data with dimension (samples, time steps, features)\n",
    "    @param y_test: (list) predict data \n",
    "    @param model_dl: (model) rnn type model (LSTM, GRU) default LSTM\n",
    "    @param unit: (int) number of cells of the RNN default 4\n",
    "    @param look_back: (int) number of values in x list, configure the shape of the RNN\n",
    "    @param cnn: (bool) add cnn part of the model default false\n",
    "    @param bidirection: (bool) add bidirectional model for the RNN default false\n",
    "    @param stacked: (bool) stacked two layers of RNN models default false\n",
    "    @return train_predict: (list) predicted values of x_train\n",
    "    @return train_y: (list) real y values \n",
    "    @return test_predict: (list) predicted test values of x_test\n",
    "    @return test_y: (list) real test values\n",
    "    @return (dataframe) containing the name of the model and the metrics\n",
    "    '''\n",
    "    # Configure an earlystopping callbacks to avoid overfitting\n",
    "    es = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='loss',  patience=5, verbose=0, mode='auto',\n",
    "    )\n",
    "    \n",
    "    # instance of a Sequential model\n",
    "    model = Sequential()\n",
    "    \n",
    "    if cnn: # test if cnn part is needed \n",
    "        print(\"CNN\")\n",
    "        model.add(Conv1D(128, 5, activation='relu'))\n",
    "        model.add(MaxPooling1D(pool_size=4))\n",
    "        model.add(Dropout(0.2))\n",
    "    \n",
    "    if not bidirection and not stacked: # test if simple model is needed\n",
    "        print(\"Simple Model\")\n",
    "        name = model_dl.__name__\n",
    "        model.add(model_dl(unit,  input_shape=(look_back, 1)))\n",
    "    elif not bidirection: # test if bidirectional  model is needed\n",
    "        print(\"Stacked Model\")\n",
    "        name = \"Stacked_\"+model_dl.__name__\n",
    "        model.add(model_dl(unit,  input_shape=(look_back, 1), return_sequences=True))\n",
    "        model.add(model_dl(unit,  input_shape=(look_back, 1)))\n",
    "    elif not stacked: # test if stacked models are needed\n",
    "        print(\"Bidirectional Model\")\n",
    "        name = \"Bi_\"+model_dl.__name__\n",
    "        model.add(Bidirectional(model_dl(unit,  input_shape=(look_back, 1))))\n",
    "    else: # test if bidirectional and stacked models are needed\n",
    "        print(\"Stacked Bidirectional Model\")\n",
    "        name = \"Stacked_Bi_\"+model_dl.__name__\n",
    "        model.add(Bidirectional(model_dl(unit,  input_shape=(look_back, 1), return_sequences=True)))\n",
    "        model.add(Bidirectional(model_dl(unit,  input_shape=(look_back, 1))))\n",
    "        \n",
    "    if cnn: # update name with cnn part \n",
    "        name = \"CNN_\"+name\n",
    "    \n",
    "    # add Dense layer with one layer and activation function linear to predict continuous values \n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam') # MSE loss but can be replace by 'mean_absolute_error'\n",
    "    model.fit(trainX, trainY, epochs=1000, batch_size=100, callbacks=[es], verbose=0)\n",
    "    \n",
    "    # make predictions\n",
    "    train_predict = model.predict(x_train)\n",
    "    test_predict = model.predict(x_test)\n",
    "    \n",
    "    # invert predictions\n",
    "    train_predict = scaler.inverse_transform(train_predict)\n",
    "    train_y = scaler.inverse_transform(y_train)\n",
    "    test_predict = scaler.inverse_transform(test_predict)\n",
    "    test_y = scaler.inverse_transform(y_test)\n",
    "    \n",
    "    # compute the metrics \n",
    "    print(\"Train\")\n",
    "    mae_train, mse_train = metrics_time_series( train_y, train_predict)\n",
    "    print(\"Test\")\n",
    "    mae_test, mse_test = metrics_time_series( test_y, test_predict)\n",
    "    \n",
    "    return train_predict, train_y, test_predict, test_y, pd.DataFrame([name, mae_train, mse_train, mae_test, mse_test], index=[\"Name\", \"mae_train\", \"mse_train\", \"mae_test\", \"mse_test\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting_predictions(dataset, look_back, train_predict,  test_predict):\n",
    "    # shift train predictions for plotting\n",
    "    trainPredictPlot = np.empty_like(dataset)\n",
    "    trainPredictPlot[:, :] = np.nan\n",
    "    trainPredictPlot[look_back:len(train_predict)+look_back, :] = train_predict\n",
    "    # shift test predictions for plotting\n",
    "    testPredictPlot = np.empty_like(dataset)\n",
    "    testPredictPlot[:, :] = np.nan\n",
    "    testPredictPlot[len(train_predict)+(look_back*2)+1:len(dataset)-1, :] = test_predict\n",
    "    # plot baseline and predictions\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.plot(scaler.inverse_transform(dataset[2000:]), 'b+--', label=\"Data\")\n",
    "    plt.plot(trainPredictPlot[2000:], 'g+-', label='Train')\n",
    "    plt.plot(testPredictPlot[2000:], 'r.--', label=\"Test\")\n",
    "    #plt.xticks(range(len(dataset[2000:])), np.array(range(len(dataset[2000:])))+2000)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    #plt.savefig(\"LSTM_predictions.png\", dpi=300)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>SimpleRNN</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train_predict_RNN, y_train_RNN,x_test_predict_RNN, y_test_RNN, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=SimpleRNN ,  unit=12, look_back=20)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_RNN,  x_test_predict_RNN)\n",
    "#df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_RNN, y_train_RNN,x_test_predict_RNN, y_test_RNN, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=SimpleRNN ,  unit=12, look_back=20, stacked=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_RNN,  x_test_predict_RNN)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>LSTM</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_lstm, y_train_lstm,x_test_predict_lstm, y_test_lstm, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=LSTM ,  unit=12, look_back=20)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_lstm,  x_test_predict_lstm)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>GRU</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import  GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_gru, y_train_gru,x_test_predict_gru, y_test_gru, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=GRU ,  unit=12, look_back=20)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_gru,  x_test_predict_gru)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>Stacked LSTMs</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_stack_lstm, y_train_stack_lstm,x_test_predict_stack_lstm, y_test_stack_lstm, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=LSTM ,  unit=12, look_back=20, stacked=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_stack_lstm,  x_test_predict_stack_lstm)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><u><h3>Stacked GRU</h3></u></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_stack_gru, y_train_stack_gru,x_test_predict_stack_gru, y_test_stack_gru, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=GRU ,  unit=12, look_back=20, stacked=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_stack_gru,  x_test_predict_stack_gru)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>Bidirectional LSTM</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_biLSTM, y_train_biLSTM,x_test_predict_biLSTM, y_test_biLSTM, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=LSTM ,  unit=12, look_back=20, stacked=False, bidirection=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_biLSTM,  x_test_predict_biLSTM)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>Bidirectional GRU</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_biGRU, y_train_biGRU,x_test_predict_biGRU, y_test_biGRU, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=GRU ,  unit=12, look_back=20, stacked=False, bidirection=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_biGRU,  x_test_predict_biGRU)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><u><h3>Stacked BiLSTM</h3></u></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_stack_biLSTM, y_train_stack_biLSTM,x_test_predict_stack_biLSTM, y_test_stack_biLSTM, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=LSTM ,  unit=12, look_back=20, stacked=True, bidirection=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_stack_biLSTM,  x_test_predict_stack_biLSTM)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><u><h3>Stacked BiGRU</h3></u></i>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_stack_biGRU, y_train_stack_biGRU,x_test_predict_stack_biGRU, y_test_stack_biGRU, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=GRU ,  unit=12, look_back=20, stacked=True, bidirection=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_stack_biGRU,  x_test_predict_stack_biGRU)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.sort_values(by=[\"mae_test\", \"mse_test\"],ascending=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>CNN-LSTM</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_cnn_lstm, y_train_cnn_lstm,x_test_predict_cnn_lstm, y_test_cnn_lstm, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=LSTM ,  unit=12, look_back=20, cnn=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_lstm,  x_test_predict_lstm)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u><i><h3>CNN-GRU</h3></i></u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_cnn_gru, y_train_cnn_gru,x_test_predict_cnn_gru, y_test_cnn_gru, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=GRU ,  unit=12, look_back=20, cnn=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_cnn_gru,  x_test_predict_cnn_gru)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_predict_stack_cnn_gru, y_train_stack_cnn_gru,x_test_predict_stack_cnn_gru, y_test_stack_cnn_gru, res= time_series_deep_learning(train_x, train_y, test_x, test_y, model_dl=GRU ,  unit=5, look_back=20, cnn=True, stacked=True)\n",
    "plotting_predictions(dataset, look_back, x_train_predict_stack_cnn_gru,  x_test_predict_stack_cnn_gru)\n",
    "df_results = df_results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
